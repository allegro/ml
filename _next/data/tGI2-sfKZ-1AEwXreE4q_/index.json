{"pageProps":{"posts":[{"guid":0,"title":"How to create a synthetic annotator? The process of developing a domain-specific LLM-as-a-Judge.","authors":[{"name":"Zuzanna Rękawek","url":"https://blog.allegro.tech/authors/zuzanna.rekawek","photo":"https://blog.allegro.tech/assets/img/authors/zuzanna.rekawek.jpg"},{"name":"Agata Hajduk-Smak","url":"https://blog.allegro.tech/authors/agata.hajduk-smak","photo":"https://blog.allegro.tech/assets/img/authors/agata.hajduk-smak.jpg"}],"link":"https://blog.allegro.tech/2025/03/how-to-create-a-synthetic-annotator.html","pubDate":"07 Mar 2025","contentSnippet":"In this blogpost we want to introduce the topic of using a Large Language Model (LLM) as an evaluator — a novel approach to tackling the complexities of evaluating advanced machine learning systems, particularly in tasks like Automatic Summarization, Text Generation, and Machine Translation, where traditional metrics struggle to capture nuances like cross-lingual accuracy and bias detection."},{"guid":1,"title":"Trust no one, not even your training data! Machine learning from noisy data","authors":[{"name":"Alicja Rączkowska","url":"https://blog.allegro.tech/authors/alicja.raczkowska","photo":"https://blog.allegro.tech/assets/img/authors/alicja.raczkowska.jpg"},{"name":"Aleksandra Osowska-Kurczab","url":"https://blog.allegro.tech/authors/aleksandra.osowska-kurczab","photo":"https://blog.allegro.tech/assets/img/authors/aleksandra.osowska-kurczab.jpg"},{"name":"Jacek Szczerbiński","url":"https://blog.allegro.tech/authors/jacek.szczerbinski","photo":"https://blog.allegro.tech/assets/img/authors/jacek.szczerbinski.jpg"},{"name":"Klaudia Nazarko","url":"https://blog.allegro.tech/authors/klaudia.nazarko","photo":"https://blog.allegro.tech/assets/img/authors/klaudia.nazarko.jpg"},{"name":"Kalina Kobus","url":"https://blog.allegro.tech/authors/kalina.kobus","photo":"https://blog.allegro.tech/assets/img/authors/kalina.kobus.jpg"}],"link":"https://blog.allegro.tech/2023/04/learning-from-noisy-data.html","pubDate":"18 Apr 2023","contentSnippet":"Label noise is ever-present in machine learning practice.\n  Allegro datasets are no exception.\n  We compared 7 methods for training classifiers robust to label noise.\n  All of them improved the model’s performance on noisy datasets.\n  Some of the methods decreased the model’s performance in the absence of label noise."},{"guid":2,"title":"Turn-Based Offline Reinforcement Learning","authors":[{"name":"Riccardo Belluzzo","url":"https://blog.allegro.tech/authors/riccardo.belluzzo","photo":"https://blog.allegro.tech/assets/img/authors/riccardo.belluzzo.jpg"},{"name":"Tomasz Bocheński","url":"https://blog.allegro.tech/authors/tomasz.bochenski","photo":"https://blog.allegro.tech/assets/img/authors/tomasz.bochenski.jpg"},{"name":"Michał Zając","url":"https://blog.allegro.tech/authors/michal.zajac","photo":"https://blog.allegro.tech/assets/img/authors/michal.zajac.jpg"},{"name":"Łukasz Kuciński","url":"https://blog.allegro.tech/authors/lukasz.kucinski","photo":"https://blog.allegro.tech/assets/img/authors/lukasz.kucinski.jpg"},{"name":"Piotr Miłoś","url":"https://blog.allegro.tech/authors/piotr.milos","photo":"https://blog.allegro.tech/assets/img/authors/piotr.milos.jpg"}],"link":"https://blog.allegro.tech/2022/04/turn-based-offline-rl.html","pubDate":"14 Apr 2022","contentSnippet":"This blogpost is the result of a research collaboration between the Allegro Machine Learning Research team and\nthe Institute of Mathematics of the Polish Academy of Sciences (IMPAN), Warsaw."}],"jobs":[],"papers":[{"authors":"Alicja Rączkowska, Riccardo Belluzzo, Piotr Zieliński, Joanna Baran, Paweł Olszewski","date":"2025","paper_url":"https://arxiv.org/abs/2507.02529","accepted_at":"arXiv","paper_title":"RetrySQL: text-to-SQL training with retry data for self-correcting query generation"},{"authors":"Alicja Rączkowska, Aleksandra Osowska-Kurczab, Jacek Szczerbiński, Kalina Jasinska-Kobus, Klaudia Nazarko","date":"2024","paper_url":"https://arxiv.org/abs/2407.10992","accepted_at":"28th International Conference on Artificial Intelligence and Statistics (AISTATS 2025)","paper_title":"AlleNoise - large-scale text classification benchmark dataset with real-world label noise"},{"authors":"Roman Dušek, Aleksander Wawer, Christopher Galias, Lidia Wojciechowska","date":"2023","paper_url":"https://annals-csis.org/Volume_35/drp/7569.html","accepted_at":"Proceedings of the 18th Conference on Computer Science and Intelligence Systems, FedCSIS 2023","paper_title":"Improving Domain-Specific Retrieval by NLI Fine-Tuning"},{"authors":"Aleksandra Chrabrowa, Tsimur Hadeliya, Dariusz Kajtoch, Robert Mroczkowski, Piotr Rybak","date":"2023","paper_url":"https://aclanthology.org/2023.findings-eacl.68/","accepted_at":"Findings of the Association for Computational Linguistics: EACL 2023","paper_title":"Going beyond research datasets: Novel intent discovery in the industry setting"},{"authors":"Aleksandra Chrabrowa, Łukasz Dragan, Karol Grzegorczyk, Dariusz Kajtoch, Mikołaj Koszowski, Robert Mroczkowski, Piotr Rybak","date":"2022","paper_url":"http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.466.pdf","accepted_at":"Proceedings of the 13th Conference on Language Resources and Evaluation (LREC 2022)","paper_title":"Evaluation of Transfer Learning for Polish with a Text-to-Text Model"},{"authors":"Mikołaj Koszowski, Karol Grzegorczyk, Tsimur Hadeliya","date":"2021","paper_url":"https://aclanthology.org/2021.wmt-1.10/","accepted_at":"Proceedings of the Sixth Conference on Machine Translation","paper_title":"Allegro.eu Submission to WMT21 News Translation Task"},{"authors":"Robert Mroczkowski, Piotr Rybak, Alina Wróblewska, Ireneusz Gawlik","date":"2021","paper_url":"https://www.aclweb.org/anthology/2021.bsnlp-1.1/","accepted_at":"BSNLP, accepted long paper","paper_title":"HerBERT: Efficiently Pretrained Transformer-based Language Model for Polish"},{"authors":"Piotr Rybak, Robert Mroczkowski, Janusz Tracz, Ireneusz Gawlik","date":"2020","paper_url":"https://www.aclweb.org/anthology/2020.acl-main.111/","accepted_at":"ACL 2020, accepted long paper","paper_title":"KLEJ: Comprehensive Benchmark for Polish Language Understanding"},{"authors":"Przemysław Pobrotyn, Tomasz Bartczak, Mikołaj Synowiec, Radosław Białobrzeski, Jarosław Bojar","date":"2020","paper_url":"https://arxiv.org/abs/2005.10084","accepted_at":"SIGIR eCommerce Workshop 2020, contributed talk","paper_title":"Context-Aware Learning to Rank with Self-Attention"},{"authors":"Przemysław Pobrotyn, Radosław Białobrzeski","date":"2020","paper_url":"https://arxiv.org/abs/2102.07831","accepted_at":"The 2021 SIGIR Workshop On eCommerce (SIGIR eCom ’21)","paper_title":"NeuralNDCG: Direct Optimisation of a Ranking Metric via Differentiable Relaxation of Sorting"}],"videos":[{"title":"Recommender Systems Unboxed: The Tech Behind Personalized Experiences","url":"https://www.youtube.com/watch?v=2B4KAlhkT0s","who":"Aleksandra Osowska-Kurczab","description":"Talk at Women in Tech Summit 2025","thumb":"images/video-headers/ola-women-in-tech-2025.png"},{"title":"AlleNoise - large-scale text classification benchmark dataset with real-world label noise","url":"https://www.youtube.com/watch?v=bZEeUeRTgz4","who":"Alicja Rączkowska","description":"Sponsored talk at MLinPL 2024","thumb":"images/video-headers/ala-mlinpl-2024.png"},{"title":"Understand, translate, respond - Clever language modeling at Allegro","url":"https://www.youtube.com/watch?v=YHLvMqi-wSo","who":"Aleksandra Osowska-Kurczab & Jacek Szczerbiński","description":"Talk at Data Science Summit Machine Learning Edition - 14.06.2024","thumb":"images/video-headers/ola-jacek-dssmle-2024.png"},{"title":"Dense Retrieval for Allegro Search Engine","url":"https://youtu.be/8CcRsX4IMnU?t=3403","who":"Aleksandra Chrabrowa","description":"Seminar at Warsaw.ai - Episode XXIII - 23.05.2024","thumb":"images/video-headers/ola-warsawai2024.png"}],"videos2":[{"title":"Machine Translation at Allegro – Use your model wise but data wiser","url":"https://www.youtube.com/watch?v=dIrCMFiyGj0","who":"Mikołaj Koszowski","description":"Seminar at Warsaw.ai - Episode XXI - 18.01.2024","thumb":"images/video-headers/miko-warsawai.png"},{"title":"Building a vector search engine","url":"https://www.youtube.com/watch?v=a-B9Rcl6-3A","who":"Maciej Mościcki","description":"MOPS Community","thumb":"images/video-headers/maciej-mlops.png"},{"title":"Architecting ML for Huge Impact and Scale","url":"https://www.youtube.com/watch?v=ESEeo_7ERRc","who":"Szymon Jacoń","description":"MOPS Community","thumb":"images/video-headers/szymon-mlops.png"},{"title":"The structure of customer service data @Allegro","url":"https://www.youtube.com/watch?v=jK-NX_qufjI","who":"Aleksandra Chrabrowa","description":"GHOST Day: AMLC 2022","thumb":"images/video-headers/ola-ghost22.png"}],"open_source":[{"name":"AlleNoise","url":"https://github.com/allegro/AlleNoise","description":"Large-scale text classification benchmark dataset with real-world label noise. It is meant to spark development of new robust classification methods."},{"name":"Hugging Face Allegro","url":"https://huggingface.co/allegro","description":"We contribute to the NLP community by publishing models and datasets to the Hugging Face Hub!\n"},{"name":"allms","url":"https://github.com/allegro/allms","description":"Versatile and powerful library designed to streamline the process of querying large language models.\n*  Simple and User-Friendly Interface,\n* Asynchronous Querying,\n* Automatic Retrying Mechanism,\n* Error Handling and Management,\n* Output Parsing"},{"name":"allRank","url":"https://github.com/allegro/allRank","description":"Framework for training neural Learning-to-Rank (LTR) models,\nfeaturing implementations of:\n*  common pointwise, pairwise and listwise loss function,\n* fully connected and Transformer-like scoring function,\n* commonly used evaluation metrics like Normalized Discounted Cumulative Gain (NDCG) and Mean Reciprocal Rank (MRR},\n* click-models for experiments on simulated click-through data"},{"name":"KLEJ Benchmark","url":"https://klejbenchmark.com/","icon":"FaTint","description":"The KLEJ benchmark (Kompleksowa Lista Ewaluacji Językowych) is a set of nine evaluation tasks for the Polish language understanding. Key benchmark features:\n* It contains a diverse set of tasks from different domains and with different objectives,\n* Most tasks are created from existing datasets but we also release the new sentiment analysis dataset from an e-commerce domain."},{"name":"HerBERT","url":"https://huggingface.co/allegro/herbert-large-cased","description":"HerBERT is a BERT-based language model trained on six different corpora for Polish language understanding. It achieves state-of-the-art results on multiple downstream tasks, including [KLEJ Benchmark](https://klejbenchmark.com/) and Part-of-Speech tagging. We release both Base and Large variants of the model as a part of [transformers](https://github.com/huggingface/transformers) library for anyone to use."}],"teams":[{"name":"Machine Translation","icon":"FaRobot","description":"We are developing an in-house Machine Translation engine specifically for e-commerce purposes, aiming to provide better value compared to off-the-shelf solutions. Our focus is on accurately translating industry-specific terms and jargon, while also creating a scalable and cost-efficient solution. We employ state-of-the-art machine learning methods, involving human evaluators and automatic quality estimation models to continually enhance translation quality. Our goal is to make our platform accessible to non-Polish speakers globally and contribute to the machine translation community."},{"name":"Language Modeling","icon":"FaSitemap","description":"We employ state-of-the-art deep learning models and a range of NLP algorithms to solve diverse problems that require semantic understanding of the specialized language used within a unique environment of an e-commerce platform. We utilize and develop Large Language Models (LLMs), with the goal of providing the company with general purpose Foundation Models that can be tailored for specific downstream tasks. On a daily basis, we use our models in the following applications: Semantic Search, Question Answering, Conversational AI, Generative AI, Named Entity Recognition."},{"name":"Learning to Rank","icon":"FaList","description":"In Learning to Rank our goal is to develop machine learning models for search. Our main focus is on ranking solutions in all phases of the search pipeline, serving millions of searches a day. Currently our main area of expertise is neural text-based search and relevance. We’re also interested in topics such as reranking, feature interaction architectures, and personalization."},{"name":"Computer Vision","icon":"FaImages","description":"At MLR Computer Vision, our primary objective is to elevate the user experience by leveraging machine learning image processing algorithms. We specifically concentrate on image representation learning for Visual Search and the development of robust image classification models. Presently, our research is focused on the integration of multiple modalities into our models. This integration enables our models to process not only images but also harness diverse sources of information such as product titles, descriptions, and attributes. The implementation of these multimodal models holds significant potential in various domains, including semantic search and the enhancement of product catalog quality. By employing such models, we aim to deliver superior solutions in these areas, ultimately providing enhanced user experiences."},{"name":"Recommendations","icon":"FaShoppingBag","description":"Our team's primary objective is to fulfill users' needs by providing them with a diverse range of products that align with their interests. We strive to inspire users and connect them with relevant offers by leveraging recommender systems. To achieve this, we rely on the collective behaviors of our user-base, forming the foundation of our algorithms. However, we also incorporate content features of the items into our models, enriching recommendations with exploratory algorithms. These algorithms not only utilize historical data but also actively engage with the world, enabling us to explore new possibilities. Our major challenges revolve around developing innovative algorithms that can deliver high-quality recommendations while effectively handling Allegro's significant daily traffic. This ambitious endeavor requires us to operate at scale, ensuring seamless user experiences across the platform."},{"name":"ML Ops","icon":"FaSignal","description":"The MLOps team aims to optimize, scale, and deploy advanced machine learning models. We blend artificial intelligence, software engineering, and DevOps expertise to embrace the full potential of research engineers and data scientists from other teams. We orchestrate the entire machine learning lifecycle, from data preprocessing and annotation to model deployment, using the cutting-edge infrastructure of Google Cloud and Kubernetes. We're operating at a massive scale with several terabytes of data processed daily and thousands of predictions per second. "}]},"__N_SSG":true}