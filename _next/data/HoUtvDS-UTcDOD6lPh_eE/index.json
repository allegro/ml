{"pageProps":{"posts":[{"title":"Trust no one, not even your training data! Machine learning from noisy data","link":"https://blog.allegro.tech/2023/04/learning-from-noisy-data.html","pubDate":"Tue, 18 Apr 2023 00:00:00 +0200","authors":{"author":[{"name":["Łukasz Rączkowski"],"photo":["https://blog.allegro.tech/img/authors/lukasz.raczkowski.jpg"],"url":["https://blog.allegro.tech/authors/lukasz.raczkowski"]},{"name":["Aleksandra Osowska-Kurczab"],"photo":["https://blog.allegro.tech/img/authors/aleksandra.osowska-kurczab.jpg"],"url":["https://blog.allegro.tech/authors/aleksandra.osowska-kurczab"]},{"name":["Jacek Szczerbiński"],"photo":["https://blog.allegro.tech/img/authors/jacek.szczerbinski.jpg"],"url":["https://blog.allegro.tech/authors/jacek.szczerbinski"]},{"name":["Klaudia Nazarko"],"photo":["https://blog.allegro.tech/img/authors/klaudia.nazarko.jpg"],"url":["https://blog.allegro.tech/authors/klaudia.nazarko"]},{"name":["Kalina Kobus"],"photo":["https://blog.allegro.tech/img/authors/kalina.kobus.jpg"],"url":["https://blog.allegro.tech/authors/kalina.kobus"]}]},"content":null,"contentSnippet":"Label noise is ever-present in machine learning practice.\nAllegro datasets are no exception.\nWe compared 7 methods for training classifiers robust to label noise.\nAll of them improved the model’s perf","guid":"https://blog.allegro.tech/2023/04/learning-from-noisy-data.html","categories":["tech","mlr","robustness","research","ml","machine-learning","ai"],"isoDate":"2023-04-17T22:00:00.000Z","thumbnail":"images/post-headers/mlr.png"},{"title":"Turn-Based Offline Reinforcement Learning","link":"https://blog.allegro.tech/2022/04/turn-based-offline-rl.html","pubDate":"Thu, 14 Apr 2022 00:00:00 +0200","authors":{"author":[{"name":["Riccardo Belluzzo"],"photo":["https://blog.allegro.tech/img/authors/riccardo.belluzzo.jpg"],"url":["https://blog.allegro.tech/authors/riccardo.belluzzo"]},{"name":["Tomasz Bocheński"],"photo":["https://blog.allegro.tech/img/authors/tomasz.bochenski.jpg"],"url":["https://blog.allegro.tech/authors/tomasz.bochenski"]},{"name":["Michał Zając"],"photo":["https://blog.allegro.tech/img/authors/michal.zajac.jpg"],"url":["https://blog.allegro.tech/authors/michal.zajac"]},{"name":["Łukasz Kuciński"],"photo":["https://blog.allegro.tech/img/authors/lukasz.kucinski.jpg"],"url":["https://blog.allegro.tech/authors/lukasz.kucinski"]},{"name":["Piotr Miłoś"],"photo":["https://blog.allegro.tech/img/authors/piotr.milos.jpg"],"url":["https://blog.allegro.tech/authors/piotr.milos"]}]},"content":null,"contentSnippet":"This blogpost is the result of a research collaboration between the Allegro Machine Learning Research team and\nthe Institute of Mathematics of the Polish Academy of Sciences (IMPAN), Warsaw.\nIntroduct","guid":"https://blog.allegro.tech/2022/04/turn-based-offline-rl.html","categories":["tech","mlr","rl","research"],"isoDate":"2022-04-13T22:00:00.000Z","thumbnail":"images/post-headers/mlr.png"}],"jobs":[],"papers":[{"authors":"Aleksandra Chrabrowa, Tsimur Hadeliya, Dariusz Kajtoch, Robert Mroczkowski, Piotr Rybak","date":"2023","paper_url":"https://aclanthology.org/2023.findings-eacl.68/","accepted_at":"Findings of the Association for Computational Linguistics: EACL 2023","paper_title":"Going beyond research datasets: Novel intent discovery in the industry setting"},{"authors":"Aleksandra Chrabrowa, Łukasz Dragan, Karol Grzegorczyk, Dariusz Kajtoch, Mikołaj Koszowski, Robert Mroczkowski, Piotr Rybak","date":"2022","paper_url":"http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.466.pdf","accepted_at":"Proceedings of the 13th Conference on Language Resources and Evaluation (LREC 2022)","paper_title":"Evaluation of Transfer Learning for Polish with a Text-to-Text Model"},{"authors":"Mikołaj Koszowski, Karol Grzegorczyk, Tsimur Hadeliya","date":"2021","paper_url":"https://aclanthology.org/2021.wmt-1.10/","accepted_at":"Proceedings of the Sixth Conference on Machine Translation","paper_title":"Allegro.eu Submission to WMT21 News Translation Task"},{"authors":"Konrad Czechowski, Tomasz Odrzygóźdź, Marek Zbysiński, Michał Zawalski, Krzysztof Olejnik, Yuhuai Wu, Łukasz Kuciński, Piotr Miłoś","date":"2021","paper_url":"https://arxiv.org/abs/2108.11204","accepted_at":"Conference and Workshop on Neural Information Processing Systems (NeurIPS)","paper_title":"Subgoal Search For Complex Reasoning Tasks"},{"authors":"Robert Mroczkowski, Piotr Rybak, Alina Wróblewska, Ireneusz Gawlik","date":"2021","paper_url":"https://www.aclweb.org/anthology/2021.bsnlp-1.1/","accepted_at":"BSNLP, accepted long paper","paper_title":"HerBERT: Efficiently Pretrained Transformer-based Language Model for Polish"},{"authors":"Piotr Rybak, Robert Mroczkowski, Janusz Tracz, Ireneusz Gawlik","date":"2020","paper_url":"https://www.aclweb.org/anthology/2020.acl-main.111/","accepted_at":"ACL 2020, accepted long paper","paper_title":"KLEJ: Comprehensive Benchmark for Polish Language Understanding"},{"authors":"Przemysław Pobrotyn, Tomasz Bartczak, Mikołaj Synowiec, Radosław Białobrzeski, Jarosław Bojar","date":"2020","paper_url":"https://arxiv.org/abs/2005.10084","accepted_at":"SIGIR eCommerce Workshop 2020, contributed talk","paper_title":"Context-Aware Learning to Rank with Self-Attention"},{"authors":"Przemysław Pobrotyn, Radosław Białobrzeski","date":"2020","paper_url":"https://arxiv.org/abs/2102.07831","accepted_at":"The 2021 SIGIR Workshop On eCommerce (SIGIR eCom ’21)","paper_title":"NeuralNDCG: Direct Optimisation of a Ranking Metric via Differentiable Relaxation of Sorting"},{"authors":"Janusz Tracz, Piotr Wójcik, Kalina Jasinska-Kobus, Riccardo Belluzzo, Robert Mroczkowski, Ireneusz Gawlik","date":"2020","paper_url":"https://www.aclweb.org/anthology/2020.ecomnlp-1.7/","accepted_at":"EComNLP 2020 COLING Workshop on Natural Language Processing in E-Commerce","paper_title":"BERT-based similarity learning for product matching"}],"videos":[{"title":"Retrieval at Scale","url":"https://www.youtube.com/watch?v=fP0e3nuUyRY","who":"Aleksandra Osowska-Kurczab & Jacek Szczerbiński","description":"Sponsored talk by Allegro for ML in PL Conference 2022","thumb":"images/video-headers/ola-jacek-mlinpl-2022.png"},{"title":"Use Your Data Wisely – Data-Centric NLP in the E-commerce Domain","url":"https://www.youtube.com/watch?v=zEgU0mIKiVA&t","who":"Paweł Olszewski","description":"Seminar at Warsaw.ai - Episode XV - 2.06.2022","thumb":"images/video-headers/pawel-warsawai.png"},{"title":"Evaluation of Transfer Learning for Polish with a Text-to-Text Model","url":"https://s3.eu-west-2.wasabisys.com/lrec2022/sessions/575.mp4","who":"Dariusz Kajtoch","description":"Paper presentation at LREC 2022","thumb":"images/video-headers/darek-lrec2022.png"},{"title":"Introduction to Offline Reinforcement Learning and its applications","url":"https://www.youtube.com/watch?v=zF8TcTgcmRM","who":"Riccardo Belluzzo","description":"Seminar at the University of Padua","thumb":"images/video-headers/riccardo-padua.png"}],"videos2":[{"title":"plT5: Universal Model For Polish Language","url":"https://www.youtube.com/watch?v=BJYL7QZD6z4","who":"Dariusz Kajtoch","description":"ML in PL 2021","thumb":"images/video-headers/darek-plt5.png"},{"title":"How to translate Allegro into foreign languages","url":"https://www.youtube.com/watch?v=6KDU5TZohpM&list=PLzveSKBX_3N7yPb4ErB5HJ83eB6XvH37C&index=27","who":"Karol Grzegorczyk","description":"ATM 2021 public track presentation","thumb":"images/video-headers/karol-atm.png"},{"title":"“Do you speak Allegro?” Large Scale Language Modeling","url":"https://www.youtube.com/watch?v=6T-R4kgIbBs&list=PLzveSKBX_3N7yPb4ErB5HJ83eB6XvH37C&index=19","who":"Riccardo Belluzzo","description":"ATM 2021 public track presentation","thumb":"images/video-headers/riccardo-atm.png"},{"title":"Polacy nie gęsi, iż swojego BERTa mają","url":"https://vimeo.com/471413175","who":"Piotr Rybak","description":"An introduction to our HerBERT model. Presentation at https://www.nlpday.pl/","thumb":"images/video-headers/polacy_nie_gesi.png"}],"open_source":[{"name":"allRank","url":"https://github.com/allegro/allRank","description":"framework for training neural Learning-to-Rank (LTR) models,\nfeaturing implementations of:\n*  common pointwise, pairwise and listwise loss function,\n* fully connected and Transformer-like scoring function,\n* commonly used evaluation metrics like Normalized Discounted Cumulative Gain (NDCG) and Mean Reciprocal Rank (MRR},\n* click-models for experiments on simulated click-through data"},{"name":"KLEJ Benchmark","url":"https://klejbenchmark.com/","icon":"FaTint","description":"The KLEJ benchmark (Kompleksowa Lista Ewaluacji Językowych) is a set of nine evaluation tasks for the Polish language understanding. Key benchmark features:\n* It contains a diverse set of tasks from different domains and with different objectives,\n* Most tasks are created from existing datasets but we also release the new sentiment analysis dataset from an e-commerce domain."},{"name":"HerBERT","url":"https://huggingface.co/allegro","description":"HerBERT is a BERT-based language model trained on six different corpora for Polish language understanding. It achieves state-of-the-art results on multiple downstream tasks, including [KLEJ Benchmark](https://klejbenchmark.com/) and Part-of-Speech tagging. We release both Base and Large variants of the model as a part of [transformers](https://github.com/huggingface/transformers) library for anyone to use."}],"teams":[{"name":"CX Robots","icon":"FaRobot","description":"We focus on using NLP models to understand and automate the communication at Allegro, e.g. automatically answer questions asked to our customer support. Our main research directions are related to pretraining and evaluating large language models, semi-supervised clustering, and human-in-the-loop NLP."},{"name":"Learning to Rank","icon":"FaList","description":"In Learning to Rank, our goal is to develop ranking models which find the optimal ordering of items for a given search results list, based on past users’ interactions. Such models constitute the final stage of Allegro’s search engine, serving millions of searches a day.\n\nSome of the research problems we tackle are:\n1. Incorporation of multimodal data (textual, visual, tabular) into an end-to-end ranking model.\n1. Search engine personalization\n1. Developing novel ranking architectures and loss functions"},{"name":"Visual Search","icon":"FaImages","description":"In Visual Search, we create machine learning models which enable us to create image embeddings suitable for similarity search. The main challenge is to make these embeddings sensitive to relevant visual traits of products like category, style, colour, pattern etc. while maintaining insensitivity to irrelevant information such as background, presence of a model, different camera angles etc."},{"name":"PCS Automation","icon":"FaShoppingBag","description":"We employ a diverse set of machine learning techniques to improve the product-based experience on Allegro. Problems that we solve include, e.g., product matching i.e., being able to infer the product being sold for a merchant-created offer, or automatic integration of product definitions from external product catalogs. Examples of our research directions include sampling methods in similarity learning or extreme classification methods."},{"name":"Recommendations","icon":"FaSitemap","description":"The main purpose of our team is to address users’ needs, show them a broad range of products they would be interested in - thus serving as an inspiration and connecting them with useful, contextual offers.\nWe ground our algorithms on previous collective behaviors of our user-base. But we also work towards incorporating content features of the items into the models. Our main challenges include building novel algorithms that can give good recommendations for our users and also operate at scale. Both being a significant endeavour, considering the sheer amount of traffic Allegro serves daily.\n\nWe focus our research on:\n1. Building item representation, that can serve as retrieval basis,\n1. Improving ways to detect user intents in clear (and useful) way,\n1. Current trends in recommender systems."},{"name":"Reinforcement Learning","icon":"FaSignal","description":"We aim to enhance various Allegro projects with exploratory algorithms, which are capable to not only exploit historical data but explore via interactions with the world. Currently, we are working on the optimization of Search Engine Marketing (SEM) and Content Optimization projects. Our main research directions include contextual bandits, A/B testing alternatives with casual impact discovery and offline RL. "}]},"__N_SSG":true}